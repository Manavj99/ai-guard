name: AI-Guard Enhanced CI/CD

on:
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
    branches: [main, master]
  push:
    branches: [main, master]
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Test mode (quick, full, security)'
        required: true
        default: 'quick'
        type: choice
        options:
        - quick
        - full
        - security

env:
  PYTHON_VERSION: '3.11'
  MIN_COVERAGE: 80
  QUALITY_THRESHOLD: 85

jobs:
  # Job 1: Quality Gates with Progress Monitoring
  quality-gates:
    name: ğŸš€ Quality Gates
    runs-on: ubuntu-latest
    
    permissions:
      contents: read
      pull-requests: read
      security-events: write
      checks: write
      actions: read

    outputs:
      quality-score: ${{ steps.quality-check.outputs.score }}
      coverage-percentage: ${{ steps.coverage-check.outputs.percentage }}
      security-issues: ${{ steps.security-scan.outputs.issues }}

    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for better analysis

      - name: ğŸ Setup Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: ğŸ“¦ Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .

      - name: ğŸ” Lint check
        id: lint
        run: |
          echo "ğŸ” Running flake8 linting..."
          flake8 src tests --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 src tests --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
          echo "âœ… Linting completed"

      - name: ğŸ§ª Type checking
        id: type-check
        run: |
          echo "ğŸ§ª Running mypy type checking..."
          mypy src --ignore-missing-imports --warn-unused-ignores
          echo "âœ… Type checking completed"

      - name: ğŸ›¡ï¸ Security scan
        id: security-scan
        run: |
          echo "ğŸ›¡ï¸ Running security scan with bandit..."
          bandit -r src -c .bandit -f json -o bandit-report.json || true
          SECURITY_ISSUES=$(python -c "import json; f=open('bandit-report.json'); data=json.load(f); print(len(data.get('results', [])))")
          echo "issues=$SECURITY_ISSUES" >> $GITHUB_OUTPUT
          echo "âœ… Security scan completed - Found $SECURITY_ISSUES issues"

      - name: ğŸ“Š Coverage analysis
        id: coverage-check
        run: |
          echo "ğŸ“Š Running tests with coverage..."
          export PYTHONPATH="$GITHUB_WORKSPACE"
          python -m pytest tests/ -v --cov=src --cov-report=xml:coverage.xml --cov-report=term-missing
          
          # Extract coverage percentage
          COVERAGE=$(python -c "
          import xml.etree.ElementTree as ET
          tree = ET.parse('coverage.xml')
          root = tree.getroot()
          coverage = float(root.attrib['line-rate']) * 100
          print(f'{coverage:.1f}')
          ")
          echo "percentage=$COVERAGE" >> $GITHUB_OUTPUT
          echo "âœ… Coverage analysis completed - $COVERAGE%"

      - name: ğŸ¯ AI-Guard quality gates
        id: quality-check
        run: |
          echo "ğŸ¯ Running AI-Guard quality gates..."
          export PYTHONPATH="$GITHUB_WORKSPACE"
          
          # Run AI-Guard with comprehensive analysis
          python -m src.ai_guard.analyzer \
            --min-cov ${{ env.MIN_COVERAGE }} \
            --skip-tests \
            --report-format json \
            --report-path quality-report.json
          
          # Extract quality score
          QUALITY_SCORE=$(python -c "
          import json
          with open('quality-report.json', 'r') as f:
              data = json.load(f)
              passed = sum(1 for gate in data['summary']['gates'] if gate['passed'])
              total = len(data['summary']['gates'])
              score = (passed / total) * 100 if total > 0 else 0
              print(f'{score:.1f}')
          ")
          echo "score=$QUALITY_SCORE" >> $GITHUB_OUTPUT
          echo "âœ… Quality gates completed - Score: $QUALITY_SCORE%"

      - name: ğŸ“‹ Generate comprehensive report
        run: |
          echo "ğŸ“‹ Generating comprehensive quality report..."
          
          cat << EOF > quality-summary.md
          # AI-Guard Quality Report
          
          ## Summary
          - **Quality Score**: ${{ steps.quality-check.outputs.score }}%
          - **Coverage**: ${{ steps.coverage-check.outputs.percentage }}%
          - **Security Issues**: ${{ steps.security-scan.outputs.issues }}
          
          ## Details
          - âœ… Linting: Passed
          - âœ… Type Checking: Passed
          - âœ… Security Scan: ${{ steps.security-scan.outputs.issues }} issues found
          - âœ… Coverage: ${{ steps.coverage-check.outputs.percentage }}% (threshold: ${{ env.MIN_COVERAGE }}%)
          
          ## Recommendations
          EOF
          
          if [ "${{ steps.coverage-check.outputs.percentage }}" -lt "${{ env.MIN_COVERAGE }}" ]; then
            echo "- âš ï¸ Increase test coverage to meet ${{ env.MIN_COVERAGE }}% threshold" >> quality-summary.md
          fi
          
          if [ "${{ steps.security-scan.outputs.issues }}" -gt 0 ]; then
            echo "- ğŸš¨ Review and fix security issues" >> quality-summary.md
          fi
          
          if [ "${{ steps.quality-check.outputs.score }}" -lt "${{ env.QUALITY_THRESHOLD }}" ]; then
            echo "- âš ï¸ Quality score below ${{ env.QUALITY_THRESHOLD }}% threshold" >> quality-summary.md
          fi

      - name: ğŸ“¤ Upload SARIF for GitHub Code Scanning
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: ai-guard.sarif

      - name: ğŸ“Š Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: quality-gates
          name: ai-guard-quality
          fail_ci_if_error: false

  # Job 2: Test Generation and Validation
  test-generation:
    name: ğŸ§ª Test Generation
    runs-on: ubuntu-latest
    needs: quality-gates
    if: always()
    
    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: ğŸ“¦ Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .

      - name: ğŸ² Generate speculative tests
        run: |
          echo "ğŸ² Generating speculative tests for changed files..."
          export PYTHONPATH="$GITHUB_WORKSPACE"
          
          # This would integrate with the test generation feature
          # python -m src.ai_guard.generators.testgen --diff-only
          echo "âœ… Test generation completed (placeholder)"

      - name: ğŸ§ª Validate generated tests
        run: |
          echo "ğŸ§ª Validating generated tests..."
          export PYTHONPATH="$GITHUB_WORKSPACE"
          
          # Run tests to ensure they pass
          python -m pytest tests/ -v --tb=short --maxfail=1
          echo "âœ… Test validation completed"

  # Job 3: Deployment and Monitoring
  deployment:
    name: ğŸš€ Deployment Check
    runs-on: ubuntu-latest
    needs: [quality-gates, test-generation]
    if: always() && github.ref == 'refs/heads/main'
    
    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: ğŸ“¦ Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .

      - name: ğŸ—ï¸ Build package
        run: |
          echo "ğŸ—ï¸ Building AI-Guard package..."
          python -m build
          echo "âœ… Package build completed"

      - name: ğŸ§ª Test package installation
        run: |
          echo "ğŸ§ª Testing package installation..."
          pip install dist/*.whl
          ai-guard --help
          echo "âœ… Package installation test passed"

      - name: ğŸ“Š Final quality report
        run: |
          echo "ğŸ“Š Final Quality Report:"
          echo "=========================="
          echo "Quality Gates: ${{ needs.quality-gates.outputs.quality-score }}%"
          echo "Coverage: ${{ needs.quality-gates.outputs.coverage-percentage }}%"
          echo "Security Issues: ${{ needs.quality-gates.outputs.security-issues }}"
          echo "Test Generation: âœ…"
          echo "Package Build: âœ…"
          echo "=========================="

  # Job 4: Status and Notifications
  status:
    name: ğŸ“Š Status Summary
    runs-on: ubuntu-latest
    needs: [quality-gates, test-generation, deployment]
    if: always()
    
    steps:
      - name: ğŸ“Š Generate status summary
        run: |
          echo "ğŸ“Š AI-Guard CI/CD Status Summary"
          echo "================================="
          echo ""
          echo "ğŸ¯ Quality Gates: ${{ needs.quality-gates.result }}"
          echo "ğŸ§ª Test Generation: ${{ needs.test-generation.result }}"
          echo "ğŸš€ Deployment: ${{ needs.deployment.result }}"
          echo ""
          echo "ğŸ“ˆ Quality Score: ${{ needs.quality-gates.outputs.quality-score }}%"
          echo "ğŸ“Š Coverage: ${{ needs.quality-gates.outputs.coverage-percentage }}%"
          echo "ğŸ›¡ï¸ Security Issues: ${{ needs.quality-gates.outputs.security-issues }}"
          echo ""
          
          # Determine overall status
          if [ "${{ needs.quality-gates.result }}" == "success" ] && \
             [ "${{ needs.test-generation.result }}" == "success" ] && \
             [ "${{ needs.deployment.result }}" == "success" ]; then
            echo "ğŸ‰ All checks passed! AI-Guard is ready for production."
            echo "âœ… Status: SUCCESS"
          else
            echo "âŒ Some checks failed. Review the logs above."
            echo "âš ï¸ Status: FAILED"
          fi
