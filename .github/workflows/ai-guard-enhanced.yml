name: AI-Guard Enhanced CI/CD

on:
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
    branches: [main, master]
  push:
    branches: [main, master]
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Test mode (quick, full, security)'
        required: true
        default: 'quick'
        type: choice
        options:
        - quick
        - full
        - security

env:
  PYTHON_VERSION: '3.11'
  MIN_COVERAGE: 80
  QUALITY_THRESHOLD: 85

jobs:
  # Job 1: Quality Gates with Progress Monitoring
  quality-gates:
    name: 🚀 Quality Gates
    runs-on: ubuntu-latest
    
    permissions:
      contents: read
      pull-requests: read
      security-events: write
      checks: write
      actions: read

    outputs:
      quality-score: ${{ steps.quality-check.outputs.score }}
      coverage-percentage: ${{ steps.coverage-check.outputs.percentage }}
      security-issues: ${{ steps.security-scan.outputs.issues }}

    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for better analysis

      - name: 🐍 Setup Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: 📦 Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .

      - name: 🔍 Lint check
        id: lint
        run: |
          echo "🔍 Running flake8 linting..."
          flake8 src tests --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 src tests --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
          echo "✅ Linting completed"

      - name: 🧪 Type checking
        id: type-check
        run: |
          echo "🧪 Running mypy type checking..."
          mypy src --ignore-missing-imports --warn-unused-ignores
          echo "✅ Type checking completed"

      - name: 🛡️ Security scan
        id: security-scan
        run: |
          echo "🛡️ Running security scan with bandit..."
          bandit -r src -c .bandit -f json -o bandit-report.json || true
          SECURITY_ISSUES=$(python -c "import json; f=open('bandit-report.json'); data=json.load(f); print(len(data.get('results', [])))")
          echo "issues=$SECURITY_ISSUES" >> $GITHUB_OUTPUT
          echo "✅ Security scan completed - Found $SECURITY_ISSUES issues"

      - name: 📊 Coverage analysis
        id: coverage-check
        run: |
          echo "📊 Running tests with coverage..."
          export PYTHONPATH="$GITHUB_WORKSPACE"
          python -m pytest tests/ -v --cov=src --cov-report=xml:coverage.xml --cov-report=term-missing
          
          # Extract coverage percentage
          COVERAGE=$(python -c "
          import xml.etree.ElementTree as ET
          tree = ET.parse('coverage.xml')
          root = tree.getroot()
          coverage = float(root.attrib['line-rate']) * 100
          print(f'{coverage:.1f}')
          ")
          echo "percentage=$COVERAGE" >> $GITHUB_OUTPUT
          echo "✅ Coverage analysis completed - $COVERAGE%"

      - name: 🎯 AI-Guard quality gates
        id: quality-check
        run: |
          echo "🎯 Running AI-Guard quality gates..."
          export PYTHONPATH="$GITHUB_WORKSPACE"
          
          # Run AI-Guard with comprehensive analysis
          python -m src.ai_guard.analyzer \
            --min-cov ${{ env.MIN_COVERAGE }} \
            --skip-tests \
            --report-format json \
            --report-path quality-report.json
          
          # Extract quality score
          QUALITY_SCORE=$(python -c "
          import json
          with open('quality-report.json', 'r') as f:
              data = json.load(f)
              passed = sum(1 for gate in data['summary']['gates'] if gate['passed'])
              total = len(data['summary']['gates'])
              score = (passed / total) * 100 if total > 0 else 0
              print(f'{score:.1f}')
          ")
          echo "score=$QUALITY_SCORE" >> $GITHUB_OUTPUT
          echo "✅ Quality gates completed - Score: $QUALITY_SCORE%"

      - name: 📋 Generate comprehensive report
        run: |
          echo "📋 Generating comprehensive quality report..."
          
          cat << EOF > quality-summary.md
          # AI-Guard Quality Report
          
          ## Summary
          - **Quality Score**: ${{ steps.quality-check.outputs.score }}%
          - **Coverage**: ${{ steps.coverage-check.outputs.percentage }}%
          - **Security Issues**: ${{ steps.security-scan.outputs.issues }}
          
          ## Details
          - ✅ Linting: Passed
          - ✅ Type Checking: Passed
          - ✅ Security Scan: ${{ steps.security-scan.outputs.issues }} issues found
          - ✅ Coverage: ${{ steps.coverage-check.outputs.percentage }}% (threshold: ${{ env.MIN_COVERAGE }}%)
          
          ## Recommendations
          EOF
          
          if [ "${{ steps.coverage-check.outputs.percentage }}" -lt "${{ env.MIN_COVERAGE }}" ]; then
            echo "- ⚠️ Increase test coverage to meet ${{ env.MIN_COVERAGE }}% threshold" >> quality-summary.md
          fi
          
          if [ "${{ steps.security-scan.outputs.issues }}" -gt 0 ]; then
            echo "- 🚨 Review and fix security issues" >> quality-summary.md
          fi
          
          if [ "${{ steps.quality-check.outputs.score }}" -lt "${{ env.QUALITY_THRESHOLD }}" ]; then
            echo "- ⚠️ Quality score below ${{ env.QUALITY_THRESHOLD }}% threshold" >> quality-summary.md
          fi

      - name: 📤 Upload SARIF for GitHub Code Scanning
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: ai-guard.sarif

      - name: 📊 Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: quality-gates
          name: ai-guard-quality
          fail_ci_if_error: false

  # Job 2: Test Generation and Validation
  test-generation:
    name: 🧪 Test Generation
    runs-on: ubuntu-latest
    needs: quality-gates
    if: always()
    
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 🐍 Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: 📦 Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .

      - name: 🎲 Generate speculative tests
        run: |
          echo "🎲 Generating speculative tests for changed files..."
          export PYTHONPATH="$GITHUB_WORKSPACE"
          
          # This would integrate with the test generation feature
          # python -m src.ai_guard.generators.testgen --diff-only
          echo "✅ Test generation completed (placeholder)"

      - name: 🧪 Validate generated tests
        run: |
          echo "🧪 Validating generated tests..."
          export PYTHONPATH="$GITHUB_WORKSPACE"
          
          # Run tests to ensure they pass
          python -m pytest tests/ -v --tb=short --maxfail=1
          echo "✅ Test validation completed"

  # Job 3: Deployment and Monitoring
  deployment:
    name: 🚀 Deployment Check
    runs-on: ubuntu-latest
    needs: [quality-gates, test-generation]
    if: always() && github.ref == 'refs/heads/main'
    
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 🐍 Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: 📦 Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .

      - name: 🏗️ Build package
        run: |
          echo "🏗️ Building AI-Guard package..."
          python -m build
          echo "✅ Package build completed"

      - name: 🧪 Test package installation
        run: |
          echo "🧪 Testing package installation..."
          pip install dist/*.whl
          ai-guard --help
          echo "✅ Package installation test passed"

      - name: 📊 Final quality report
        run: |
          echo "📊 Final Quality Report:"
          echo "=========================="
          echo "Quality Gates: ${{ needs.quality-gates.outputs.quality-score }}%"
          echo "Coverage: ${{ needs.quality-gates.outputs.coverage-percentage }}%"
          echo "Security Issues: ${{ needs.quality-gates.outputs.security-issues }}"
          echo "Test Generation: ✅"
          echo "Package Build: ✅"
          echo "=========================="

  # Job 4: Status and Notifications
  status:
    name: 📊 Status Summary
    runs-on: ubuntu-latest
    needs: [quality-gates, test-generation, deployment]
    if: always()
    
    steps:
      - name: 📊 Generate status summary
        run: |
          echo "📊 AI-Guard CI/CD Status Summary"
          echo "================================="
          echo ""
          echo "🎯 Quality Gates: ${{ needs.quality-gates.result }}"
          echo "🧪 Test Generation: ${{ needs.test-generation.result }}"
          echo "🚀 Deployment: ${{ needs.deployment.result }}"
          echo ""
          echo "📈 Quality Score: ${{ needs.quality-gates.outputs.quality-score }}%"
          echo "📊 Coverage: ${{ needs.quality-gates.outputs.coverage-percentage }}%"
          echo "🛡️ Security Issues: ${{ needs.quality-gates.outputs.security-issues }}"
          echo ""
          
          # Determine overall status
          if [ "${{ needs.quality-gates.result }}" == "success" ] && \
             [ "${{ needs.test-generation.result }}" == "success" ] && \
             [ "${{ needs.deployment.result }}" == "success" ]; then
            echo "🎉 All checks passed! AI-Guard is ready for production."
            echo "✅ Status: SUCCESS"
          else
            echo "❌ Some checks failed. Review the logs above."
            echo "⚠️ Status: FAILED"
          fi
